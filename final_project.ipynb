{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b63184a",
   "metadata": {},
   "source": [
    "# Databases for Analytics Final Project\n",
    "## Analysis of Airline Efficiency Data for October 2025\n",
    "### Created by Kevin Hennelly \n",
    "#### February 23, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## PROJECT OVERVIEW\n",
    "In this project I analyzed US commercial flight performance data using a dimensional (star schema) data warehouse design.  The goal is to evaluate airline performance through metrics such as:\n",
    "- Average departure delay\n",
    "- Average arrival delay\n",
    "- Total cancellations\n",
    "- Total diversions\n",
    "\n",
    "I also evaluate arrival delays based on airport.  I did not account for sample size or only focus on aiports having a minimum number of flights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236b430",
   "metadata": {},
   "source": [
    "## 1. INITIAL DATA SOURCE\n",
    "https://www.transtats.bts.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9c14a",
   "metadata": {},
   "source": [
    "## 2 DATA FORMAT\n",
    "\n",
    "The source data was provided in CSV format from the U.S. Bureau of Transportation Statistics (BTS) On-Time Reporting dataset for October 2025, which at the time of beginning this project was the most recent data on file.\n",
    "\n",
    "Each record in the CSV represents one scheduled commercial flight. The original files contain a large number of attributes, including operational timestamps, airport identifiers, aircraft identifiers, and detailed delay metrics.\n",
    "\n",
    "For this project, the raw data was transformed and loaded into a PostgreSQL data warehouse using a star schema design.\n",
    "\n",
    "\n",
    "Raw Data Characteristics\n",
    "\n",
    "    • Format: CSV\n",
    "    • Structure: One row per scheduled flight\n",
    "    • Columns: 26 attributes per flight record\n",
    "\n",
    "\n",
    "Data Warehouse (PostgreSQL) Characteristics\n",
    "\n",
    "    • Schema: analytics\n",
    "    • Fact Table: analytics.fact_flight\n",
    "    • Columns in fact table: 11\n",
    "    • Rows in fact table: 605844\n",
    "    • Grain: One row per scheduled flight\n",
    "\n",
    "Only attributes relevant to delay analysis and airline performance were retained in the fact table. Descriptive attributes such as carrier information and date components were separated into dimension tables to support aggregation and normalization.  There are four dimension tables: dim_airline, dim_airport, dim_carrier, dime_date.\n",
    "\n",
    "\n",
    "![Row Count Screenshot](Screenshots/number_of_rows.png)\n",
    "\n",
    "\n",
    "![Column Count Screenshots](Screenshots/number_of_columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b322a",
   "metadata": {},
   "source": [
    "## 3. DATA DICTIONARY\n",
    "\n",
    "![Data Dictionary Screenshot](Screenshots/data_dictionary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2532108",
   "metadata": {},
   "source": [
    "## 4. OBSTACLES OVERCOME DURING DATA TRANSFORMATION\n",
    "\n",
    "During transformation, I adressed a number of issues including: missing data values for some canceled flights; I converted text fields to numeric and data types; I created a star schema out of the downloaded flat data set, and established the primary and foreign key relationships.  Also, while I didn't download all data available I did download more than I needed.  I further narrowed the data down by only selecting the exact sets I needed to run my proposed queries.  I still think the large CSV was approporiate as it allows for other queries down the road.  Better to have data I could possibly need in one place, but only pull what I need for the current analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc19083",
   "metadata": {},
   "source": [
    "## 5. TABLE STRUCTURE\n",
    "\n",
    "![Table Structure Fact Flight](Screenshots/table_structure_fact_flight.png)\n",
    "![Table Structure Dimension Airline](Screenshots/table_structure_dim_airline.png)\n",
    "![Table Structure Dimension Airport](Screenshots/table_structure_dim_airport.png)\n",
    "![Table Structure Dimension Carriers](Screenshots/table_structure_dim_carrier.png)\n",
    "![Table Structure Dimensions Date](Screenshots/table_structure_dim_date.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a1e26",
   "metadata": {},
   "source": [
    "## 6. SELECT * RESULTS OF TABLES\n",
    "\n",
    "![Select Fact Flight](Screenshots/Select_fact_flight.png)\n",
    "![Select Dimension Airport](Screenshots/Select_dim_airport.png)\n",
    "![Select Dimension Airline](Screenshots/Select_dim_airline.png)\n",
    "![Select Dimension Carrier](Screenshots/Select_dim_carrier.png)\n",
    "![Select Dimension Date](Screenshots/Select_dim_date.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a6fce",
   "metadata": {},
   "source": [
    "## 7. QUERIES\n",
    "\n",
    "### Average Departure Delays, Average Arrival Delays, Total Cancelled, Total Diverted - by Carrier.  Ordered by Average Departure Delays.  Shows JOIN and AGGREGATE \n",
    "![Query 1](Screenshots/Query%201.png)\n",
    "\n",
    "---\n",
    "\n",
    "### JOIN flight and aiport data and sorts in descending order from longest average delay.\n",
    "![Delays](Screenshots/Destination%20Delays.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
